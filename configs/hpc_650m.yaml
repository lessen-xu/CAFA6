# CAFA6 HPC Training Configuration - ESM2 650M Model
# Location: configs/hpc_650m.yaml

data:
  data_dir: "data"
  max_length: 1024  # Increased from 512 for better coverage
  val_ratio: 0.1
  num_workers: 8  # HPC typically has more CPU cores

model:
  type: "esm"  # Full ESM model (not light)
  name: "facebook/esm2_t33_650M_UR50D"  # 650M parameters
  dropout: 0.1
  freeze_backbone: false
  use_gradient_checkpointing: true  # Save memory

training:
  batch_size: 4  # Smaller batch due to larger model
  gradient_accumulation_steps: 8  # Effective batch = 4 * 8 = 32
  epochs: 20
  learning_rate: 5.0e-5  # Lower LR for larger model
  weight_decay: 0.01
  max_grad_norm: 1.0
  warmup_ratio: 0.1
  use_amp: true  # Mixed precision training
  early_stopping_patience: 5
  use_weighted_loss: true  # Use IA-weighted BCE loss

evaluation:
  eval_every_n_epochs: 1
  threshold_search: true  # Search optimal threshold
  threshold_range: [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]

output:
  save_dir: "outputs/hpc_650m"
  save_every_n_epochs: 2
  log_every_n_steps: 50

wandb:
  enabled: true
  project: "cafa6"
  name: "esm2_650m_hpc"

